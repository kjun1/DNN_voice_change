{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import librosa\n",
    "import librosa.filters\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from scipy import interpolate\n",
    "import pyworld, pysptk\n",
    "from nnmnkwii.metrics import melcd\n",
    "from fastdtw import fastdtw\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio\n",
    "\n",
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base1 = os.path.expanduser(\"../../dataset/wav_data/uemura_normal/\")\n",
    "base2 = os.path.expanduser(\"../../dataset/wav_data/tsuchiya_normal/\")\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "# from https://github.com/keithito/tacotron/blob/08989cc3553b3a916a31f565e4f20e34bf19172f/hparams.py\n",
    "hparams = AttrDict(\n",
    "    # Audio:\n",
    "    num_mels=80,\n",
    "    num_freq=1025,\n",
    "    sample_rate=24000,\n",
    "    frame_length_ms=50,\n",
    "    frame_shift_ms=12.5,\n",
    "    preemphasis=0.97,\n",
    "    min_level_db=-100,\n",
    "    ref_level_db=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://github.com/keithito/tacotron/blob/53a840c030a1899a0510da4965d96b53a29d6679/util/audio.py\n",
    "\n",
    "def load_wav(path):\n",
    "    w = librosa.core.load(path, sr=hparams.sample_rate)[0]\n",
    "    w = librosa.effects.remix(w, intervals=librosa.effects.split(w, top_db=25))\n",
    "#     w = librosa.effects.trim(w)[0]\n",
    "    return w\n",
    "\n",
    "\n",
    "def save_wav(wav, path):\n",
    "    wav *= 32767 / max(0.01, np.max(np.abs(wav)))\n",
    "    sf.write(path, wav, 44100, 'PCM_24')\n",
    "\n",
    "\n",
    "def preemphasis(x):\n",
    "    return signal.lfilter([1, -hparams.preemphasis], [1], x)\n",
    "\n",
    "\n",
    "def inv_preemphasis(x):\n",
    "    return signal.lfilter([1], [1, -hparams.preemphasis], x)\n",
    "\n",
    "\n",
    "def spectrogram(y):\n",
    "    D = _stft(preemphasis(y))\n",
    "    S = _amp_to_db(np.abs(D)) - hparams.ref_level_db\n",
    "    return _normalize(S)\n",
    "\n",
    "\n",
    "def inv_spectrogram(spectrogram):\n",
    "    '''Converts spectrogram to waveform using librosa'''\n",
    "    S = _db_to_amp(_denormalize(spectrogram) + hparams.ref_level_db)    # Convert back to linear\n",
    "    return inv_preemphasis(_griffin_lim(S ** hparams.power))                    # Reconstruct phase\n",
    "\n",
    "\n",
    "def inv_spectrogram_tensorflow(spectrogram):\n",
    "    '''Builds computational graph to convert spectrogram to waveform using TensorFlow.\n",
    "    Unlike inv_spectrogram, this does NOT invert the preemphasis. The caller should call\n",
    "    inv_preemphasis on the output after running the graph.\n",
    "    '''\n",
    "    S = _db_to_amp_tensorflow(_denormalize_tensorflow(spectrogram) + hparams.ref_level_db)\n",
    "    return _griffin_lim_tensorflow(tf.pow(S, hparams.power))\n",
    "\n",
    "\n",
    "def melspectrogram(y):\n",
    "    D = _stft(preemphasis(y))\n",
    "    S = _amp_to_db(_linear_to_mel(np.abs(D)))\n",
    "    return _normalize(S)\n",
    "\n",
    "\n",
    "def find_endpoint(wav, threshold_db=-40, min_silence_sec=0.8):\n",
    "    window_length = int(hparams.sample_rate * min_silence_sec)\n",
    "    hop_length = int(window_length / 4)\n",
    "    threshold = _db_to_amp(threshold_db)\n",
    "    for x in range(hop_length, len(wav) - window_length, hop_length):\n",
    "        if np.max(wav[x:x+window_length]) < threshold:\n",
    "            return x + hop_length\n",
    "    return len(wav)\n",
    "\n",
    "\n",
    "def _griffin_lim(S):\n",
    "    '''librosa implementation of Griffin-Lim\n",
    "    Based on https://github.com/librosa/librosa/issues/434\n",
    "    '''\n",
    "    angles = np.exp(2j * np.pi * np.random.rand(*S.shape))\n",
    "    S_complex = np.abs(S).astype(np.complex)\n",
    "    y = _istft(S_complex * angles)\n",
    "    for i in range(hparams.griffin_lim_iters):\n",
    "        angles = np.exp(1j * np.angle(_stft(y)))\n",
    "        y = _istft(S_complex * angles)\n",
    "    return y\n",
    "\n",
    "\n",
    "def _griffin_lim_tensorflow(S):\n",
    "    '''TensorFlow implementation of Griffin-Lim\n",
    "    Based on https://github.com/Kyubyong/tensorflow-exercises/blob/master/Audio_Processing.ipynb\n",
    "    '''\n",
    "    with tf.variable_scope('griffinlim'):\n",
    "        # TensorFlow's stft and istft operate on a batch of spectrograms; create batch of size 1\n",
    "        S = tf.expand_dims(S, 0)\n",
    "        S_complex = tf.identity(tf.cast(S, dtype=tf.complex64))\n",
    "        y = _istft_tensorflow(S_complex)\n",
    "        for i in range(hparams.griffin_lim_iters):\n",
    "            est = _stft_tensorflow(y)\n",
    "            angles = est / tf.cast(tf.maximum(1e-8, tf.abs(est)), tf.complex64)\n",
    "            y = _istft_tensorflow(S_complex * angles)\n",
    "        return tf.squeeze(y, 0)\n",
    "\n",
    "\n",
    "def _stft(y):\n",
    "    n_fft, hop_length, win_length = _stft_parameters()\n",
    "    return librosa.stft(y=y, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n",
    "\n",
    "\n",
    "def _istft(y):\n",
    "    _, hop_length, win_length = _stft_parameters()\n",
    "    return librosa.istft(y, hop_length=hop_length, win_length=win_length)\n",
    "\n",
    "\n",
    "def _stft_tensorflow(signals):\n",
    "    n_fft, hop_length, win_length = _stft_parameters()\n",
    "    return tf.contrib.signal.stft(signals, win_length, hop_length, n_fft, pad_end=False)\n",
    "\n",
    "\n",
    "def _istft_tensorflow(stfts):\n",
    "    n_fft, hop_length, win_length = _stft_parameters()\n",
    "    return tf.contrib.signal.inverse_stft(stfts, win_length, hop_length, n_fft)\n",
    "\n",
    "\n",
    "def _stft_parameters():\n",
    "    n_fft = (hparams.num_freq - 1) * 2\n",
    "    hop_length = int(hparams.frame_shift_ms / 1000 * hparams.sample_rate)\n",
    "    win_length = int(hparams.frame_length_ms / 1000 * hparams.sample_rate)\n",
    "    return n_fft, hop_length, win_length\n",
    "\n",
    "\n",
    "# Conversions:\n",
    "\n",
    "_mel_basis = None\n",
    "\n",
    "def _linear_to_mel(spectrogram):\n",
    "    global _mel_basis\n",
    "    if _mel_basis is None:\n",
    "        _mel_basis = _build_mel_basis()\n",
    "    return np.dot(_mel_basis, spectrogram)\n",
    "\n",
    "def _build_mel_basis():\n",
    "    n_fft = (hparams.num_freq - 1) * 2\n",
    "    return librosa.filters.mel(hparams.sample_rate, n_fft, n_mels=hparams.num_mels)\n",
    "\n",
    "def _amp_to_db(x):\n",
    "    return 20 * np.log10(np.maximum(1e-5, x))\n",
    "\n",
    "def _db_to_amp(x):\n",
    "    return np.power(10.0, x * 0.05)\n",
    "\n",
    "def _db_to_amp_tensorflow(x):\n",
    "    return tf.pow(tf.ones(tf.shape(x)) * 10.0, x * 0.05)\n",
    "\n",
    "def _normalize(S):\n",
    "    return np.clip((S - hparams.min_level_db) / -hparams.min_level_db, 0, 1)\n",
    "\n",
    "def _denormalize(S):\n",
    "    return (np.clip(S, 0, 1) * -hparams.min_level_db) + hparams.min_level_db\n",
    "\n",
    "def _denormalize_tensorflow(S):\n",
    "    return (tf.clip_by_value(S, 0, 1) * -hparams.min_level_db) + hparams.min_level_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = 48000\n",
    "fftlen = pyworld.get_cheaptrick_fft_size(fs)\n",
    "alpha = pysptk.util.mcepalpha(fs)\n",
    "order = 25\n",
    "frame_period = 5\n",
    "hop_length = int(fs * (frame_period * 0.001))\n",
    "\n",
    "def collect_features(x, fs):\n",
    "    x = x.astype(np.float64)\n",
    "    f0, timeaxis = pyworld.dio(x, fs, frame_period=frame_period)\n",
    "    f0 = pyworld.stonemask(x, f0, timeaxis, fs)\n",
    "    spectrogram = pyworld.cheaptrick(x, f0, timeaxis, fs)\n",
    "    ap = pyworld.d4c(x, f0, timeaxis, fs)\n",
    "    return f0, spectrogram.T, ap\n",
    "\n",
    "def synth_wav(f0, sp, ap, fs):\n",
    "    return pw.synthesize(f0, sp.T, ap, fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract feature with WORLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(787,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fre1 = []\n",
    "fre2 = []\n",
    "ap1 = []\n",
    "ap2 = []\n",
    "spec1 = []\n",
    "spec2 = []\n",
    "\n",
    "for i in range(5):\n",
    "    p = glob.glob(os.path.join(base1, '*{0:03}*'.format(i+1)))[0]\n",
    "    w = load_wav(p)\n",
    "    f0, sp, ap =  collect_features(w, fs)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from scipy.stats import zscore\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mel1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2eac236b3169>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m X_train, X_test, y_train, y_test = train_test_split(\n\u001b[0;32m----> 2\u001b[0;31m     mel1, mel2, test_size=1/5, random_state=0) #random_stateは乱数シードの固定\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mel1' is not defined"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    feature1, feature2, test_size=1/5, random_state=0) #random_stateは乱数シードの固定\n",
    "\n",
    "X_train = np.hstack(X_train)\n",
    "y_train = np.hstack(y_train)\n",
    "X_test = np.hstack(X_test)\n",
    "y_test = np.hstack(y_test)\n",
    "\n",
    "a = np.concatenate([X_train, X_test, y_train, y_test],1)\n",
    "_std = a.std()\n",
    "_mean = a.mean()\n",
    "\n",
    "X_train = torch.Tensor((X_train - _mean)/_std)\n",
    "y_train = torch.Tensor((y_train - _mean)/_std)\n",
    "X_test = torch.Tensor((X_test - _mean)/_std)\n",
    "y_test = torch.Tensor((y_test - _mean)/_std)\n",
    "\n",
    "ds_train = TensorDataset(X_train, y_train) # 入力データと教師データをまとめる\n",
    "ds_test = TensorDataset(X_test, y_test) # 上同様\n",
    "\n",
    "dataloader_train = DataLoader(ds_train,batch_size=1024, shuffle=True)\n",
    "dataloader_test = DataLoader(ds_test, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
